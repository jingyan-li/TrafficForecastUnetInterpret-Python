{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bc5d836e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import json\n",
    "import pickle\n",
    "import os\n",
    "import sys\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import Polygon\n",
    "\n",
    "import time as T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0f41541e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_road_mask(MASK_PATH):\n",
    "    # Load Road Mask\n",
    "    ROAD_MASK = True\n",
    "    if ROAD_MASK:\n",
    "        # Road Mask\n",
    "        road = pickle.load(open(MASK_PATH, \"rb\"))\n",
    "        road_ = np.pad(road, pad_width=((8, 9), (6, 6)))\n",
    "    return road_\n",
    "\n",
    "def interpolate_coords(attr_shape):\n",
    "    # Bounding box to interpolate coordinates\n",
    "    yMin = 52.359 - 0.009\n",
    "    yMax = 52.854 + 0.008\n",
    "    xMin = 13.189 - 0.006\n",
    "    xMax = 13.625 + 0.006\n",
    "\n",
    "#     xtranslate = (xMax-xMin)/attr_shape[-1]\n",
    "#     ytranslate = (yMax-yMin)/attr_shape[-2]\n",
    "#     x = np.linspace(xMin,xMax,attr_shape[-1])-xtranslate # Lon\n",
    "#     y = np.linspace(yMin,yMax,attr_shape[-2])[::-1]-ytranslate # Lat\n",
    "\n",
    "    x = np.linspace(xMin,xMax,attr_shape[-1]) # Lon\n",
    "    y = np.linspace(yMin,yMax,attr_shape[-2])[::-1] # Lat\n",
    "\n",
    "    xv, yv = np.meshgrid(x,y,indexing='xy')\n",
    "    xv = np.round(xv,3)\n",
    "    yv = np.round(yv,3)\n",
    "    \n",
    "    return xv, yv\n",
    "\n",
    "# Split attribution by time epoch and volume/speed/incident level\n",
    "def spatial_attribution_agg(attr):\n",
    "    '''\n",
    "    An spatial attribution aggregate by time epoch and volume/speed/incident level\n",
    "    return: np.array.shape = [12,3,512,448] 12 time epochs, 3 represents volume/speed/incident level at the time epoch\n",
    "    '''\n",
    "    attr_all = np.zeros((12,3,attr.shape[-2],attr.shape[-1]))\n",
    "    for i in range(12):\n",
    "        start_epoch = i\n",
    "        # Non-static features per time epoch\n",
    "        x = attr[0, start_epoch*9:(start_epoch+1)*9, :, :] \n",
    "\n",
    "        # Agg speed/volume per direction\n",
    "        volume_idx = np.arange(0, 8, 2)\n",
    "        speed_idx = np.arange(1, 8, 2)\n",
    "        volume = np.mean(x[volume_idx, :, :], axis=0)[None,...]\n",
    "        speed = np.mean(x[speed_idx, :, :], axis=0)[None,...]\n",
    "        # Incident per time epoch\n",
    "        incident = x[-1, :, :][None,...]\n",
    "        # Attribution for this Time epoch\n",
    "        attr_all[i] = np.concatenate((volume,speed,incident))[None,...]\n",
    "    return attr_all\n",
    "\n",
    "def spatial_attribution_agg_flat(attr):\n",
    "    '''\n",
    "    An spatial attribution aggregate by time epoch and volume/speed/incident level, and static features\n",
    "    return: np.array.shape = [43,512,448] = [C,H,W]; last 7 in dim=0 are static features\n",
    "    '''\n",
    "    attr_ts = spatial_attribution_agg(attr)\n",
    "    attr_ts = attr_ts.reshape(-1, attr_ts.shape[-2], attr_ts.shape[-1])\n",
    "    static = attr[0, -7:, :, :]\n",
    "    attr_all = np.concatenate((attr_ts,static))\n",
    "    return attr_all\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def zoom_to_range(window, window_size, zoom_window_num, arr_shape):\n",
    "    '''\n",
    "        Zoom to a range\n",
    "    '''\n",
    "    # Zoom to the range\n",
    "    zoom_window_num_y = zoom_window_num + 2\n",
    "    start_x = (window[0]-zoom_window_num)*window_size\n",
    "    end_x = (window[0]+zoom_window_num)*window_size\n",
    "    start_y = (window[1]-zoom_window_num_y)*window_size\n",
    "    end_y = (window[1]+zoom_window_num_y)*window_size\n",
    "    start_x = start_x if start_x>0 else 0\n",
    "    end_x = end_x if end_x<arr_shape[-2] else arr_shape[-2]\n",
    "    start_y = start_y if start_x>0 else 0\n",
    "    end_y = end_y if end_y<arr_shape[-1] else arr_shape[-1]\n",
    "    return (start_x, end_x, start_y, end_y)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdb4cd56",
   "metadata": {},
   "source": [
    "## Save as geojson polygons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8d417480",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: err-pred-2019-08-10-84\n",
      "Processing: gt-inc-2019-08-10-84\n",
      "Processing: w[12, 12]-2019-08-10-84\n",
      "(86, 512, 448)\n",
      "zoom to [210:294, 168:336]\n",
      "(86, 84, 168)\n",
      "(84, 168)\n",
      "Saving used 11.641252517700195 s\n"
     ]
    }
   ],
   "source": [
    "log_root = r\"C:\\Users\\jingyli\\OwnDrive\\IPA\\attribution_Result\\unet\\attribution_pickle\\resUnet\"\n",
    "MASK_PATH = r\"C:\\Users\\jingyli\\OwnDrive\\IPA\\python-eda-code\\utils\\Berlin.mask\"\n",
    "output_root = r\"C:\\Users\\jingyli\\OwnDrive\\IPA\\web_json\"\n",
    "window_size = 21\n",
    "\n",
    "def save_window_to_geojson(window, date, time):\n",
    "    print(f\"Processing: w{window}-{date}-{time}\")\n",
    "    file_path = f\"{date}_berlin_9ch{time}-saliency-target-channel0-W{window[0]}-{window[1]}.npy\"\n",
    "    file1_path = f\"{date}_berlin_9ch{time}-saliency-target-channel1-W{window[0]}-{window[1]}.npy\"\n",
    "    # OUTPUT PATH\n",
    "    output_path = os.path.join(output_root, f\"{date}_{time}\")\n",
    "    if not os.path.exists(output_path):\n",
    "        os.mkdir(output_path)\n",
    "    output_filename = os.path.join(output_path, f\"{date}_{time}_W{window[0]}-{window[1]}.geo.json\")\n",
    "    # Test if file existis\n",
    "    if os.path.isfile(output_filename):\n",
    "        return\n",
    "\n",
    "    # Attribution to volume / speed\n",
    "    attr = np.load(os.path.join(log_root, f\"{date}_{time}\", file_path))\n",
    "    attr1 = np.load(os.path.join(log_root, f\"{date}_{time}\", file1_path))\n",
    "\n",
    "    # Aggregate attr\n",
    "    attr_all = spatial_attribution_agg_flat(attr)\n",
    "    attr_all1 = spatial_attribution_agg_flat(attr1)\n",
    "\n",
    "    ## Add up volume/speed\n",
    "    attr_all = np.concatenate([attr_all,attr_all1])\n",
    "    print(attr_all.shape)\n",
    "\n",
    "    # Zoom to a small area\n",
    "    zoom_window_num = 2\n",
    "    (start_x, end_x, start_y, end_y) = zoom_to_range(window, window_size, zoom_window_num, attr_all.shape)\n",
    "    print(f\"zoom to [{start_x}:{end_x}, {start_y}:{end_y}]\")\n",
    "    attr_zoom = attr_all[:, \n",
    "                         start_x : end_x,\n",
    "                         start_y : end_y]\n",
    "    print(attr_zoom.shape)\n",
    "\n",
    "    # Load Road mask\n",
    "    road_ = load_road_mask(MASK_PATH)\n",
    "\n",
    "    road_zoom = road_[start_x : end_x,\n",
    "                         start_y : end_y]\n",
    "    road_zoom = road_zoom.astype(np.int8)\n",
    "    print(road_zoom.shape)\n",
    "\n",
    "    # Interpolate Coords\n",
    "    xv, yv = interpolate_coords((1, 115, 512, 448))\n",
    "\n",
    "    # Apply road mask; \n",
    "    xv_zoom = xv[start_x : end_x,\n",
    "                         start_y : end_y]\n",
    "    yv_zoom = yv[start_x : end_x,\n",
    "                         start_y : end_y]\n",
    "    mask = road_zoom ==1\n",
    "    output = np.concatenate([xv_zoom[mask][:,None],\n",
    "                             yv_zoom[mask][:,None],\n",
    "                            ], axis=1)\n",
    "    output_attr = attr_zoom[:,mask].transpose()\n",
    "    # Loop through grids and create polygons\n",
    "    polygons = []\n",
    "    for rec in output:\n",
    "        x_left = rec[0]\n",
    "        y_up = rec[1]\n",
    "        x_right = x_left + 0.001\n",
    "        y_bot = y_up - 0.001\n",
    "        attr = rec[2:]\n",
    "        polygon = Polygon([(x_left,y_up), (x_right, y_up), (x_right,y_bot),(x_left, y_bot)])\n",
    "        polygons.append(polygon)\n",
    "\n",
    "    # Create geo dataframe\n",
    "    gdf_data = {'geometry':polygons}\n",
    "    for i in range(output_attr.shape[1]):\n",
    "        gdf_data[i] = output_attr[:,i]\n",
    "    grid = gpd.GeoDataFrame(gdf_data,crs=\"EPSG:4326\")\n",
    "\n",
    "    st = T.time()\n",
    "    # Save\n",
    "    data = grid.to_json(show_bbox=False, na=\"drop\")\n",
    "    with open(output_filename,\"w\") as f:\n",
    "        f.writelines(data)\n",
    "    print(f\"Saving used {T.time()-st} s\")\n",
    "    \n",
    "\n",
    "def save_stable_to_geojson(date, time, stable_name):\n",
    "    print(f\"Processing: {stable_name}-{date}-{time}\")\n",
    "    # prediction and error\n",
    "    result_path = f\"{date}_berlin_9ch{time}-{stable_name}.npy\"\n",
    "    result = np.load(os.path.join(log_root, f\"{date}_{time}\", result_path))\n",
    "    \n",
    "    # OUTPUT PATH\n",
    "    output_path = os.path.join(output_root, f\"{date}_{time}\")\n",
    "    if not os.path.exists(output_path):\n",
    "        os.mkdir(output_path)\n",
    "    output_result_path = os.path.join(output_path, f\"{date}_{time}_{stable_name}.geo.json\")\n",
    "    \n",
    "    # Test if file existis\n",
    "    if os.path.isfile(output_result_path):\n",
    "        return\n",
    "    \n",
    "    # Load Road mask\n",
    "    road_ = load_road_mask(MASK_PATH).astype(np.int8)\n",
    "\n",
    "    # Interpolate Coords\n",
    "    xv, yv = interpolate_coords((1, 115, 512, 448))\n",
    "\n",
    "\n",
    "    mask = road_ ==1\n",
    "    output = np.concatenate([xv[mask][:,None],\n",
    "                             yv[mask][:,None],\n",
    "                            ], axis=1)\n",
    "    output_result = result[:,mask].transpose()\n",
    "\n",
    "    # Loop through grids and create polygons\n",
    "    polygons = []\n",
    "    for rec in output:\n",
    "        x_left = rec[0]\n",
    "        y_up = rec[1]\n",
    "        x_right = x_left + 0.001\n",
    "        y_bot = y_up - 0.001\n",
    "        attr = rec[2:]\n",
    "        polygon = Polygon([(x_left,y_up), (x_right, y_up), (x_right,y_bot),(x_left, y_bot)])\n",
    "        polygons.append(polygon)\n",
    "\n",
    "    # Create geo dataframe\n",
    "    gdf_data = {'geometry':polygons}\n",
    "    for i in range(output_result.shape[1]):\n",
    "        gdf_data[i] = output_result[:,i]\n",
    "    grid = gpd.GeoDataFrame(gdf_data,crs=\"EPSG:4326\")\n",
    "\n",
    "    st = T.time()\n",
    "    # Save\n",
    "    data = grid.to_json(show_bbox=False, na=\"drop\")\n",
    "    with open(output_result_path,\"w\") as f:\n",
    "        f.writelines(data)\n",
    "    print(f\"Saving used {T.time()-st} s\")\n",
    "    \n",
    "    \n",
    "# times = [84,132,204] \n",
    "# dates = [\"2019-09-19\",\"2019-10-19\"]\n",
    "dates = [\"2019-08-10\"]\n",
    "times = [84]\n",
    "\n",
    "for time in times:\n",
    "    for date in dates:\n",
    "#         time = 84\n",
    "#         date = \"2019-09-19\"\n",
    "\n",
    "        save_stable_to_geojson(date, time, \"err-pred\")\n",
    "        save_stable_to_geojson(date, time, \"gt-inc\")\n",
    "\n",
    "        window_centers = [[12,12],] # [y,x][16,4],[14,5]\n",
    "        window_num = 1\n",
    "        for window in window_centers:  \n",
    "#             y = np.arange(window[0]-window_num, window[0]+window_num+1)\n",
    "#             x = np.arange(window[1]-window_num, window[1]+window_num+1)\n",
    "            y = [window[0]]\n",
    "            x = [window[1]]\n",
    "            for y_ in y:\n",
    "                for x_ in x:\n",
    "                    save_window_to_geojson([y_,x_], date, time)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60140ef6",
   "metadata": {},
   "source": [
    "v_pred,\n",
    "s_pred,\n",
    "v_gt_epoch-v_pred,\n",
    "s_gt_epoch-s_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "78f4c051",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving used 24.51560664176941 s\n"
     ]
    }
   ],
   "source": [
    "# prediction and error\n",
    "result_path = f\"{date}_berlin_9ch{time}-err-pred.npy\"\n",
    "result = np.load(os.path.join(log_root, f\"{date}_{time}\", result_path))\n",
    "output_result_path =  os.path.join(output_root, f\"{date}_{time}\", f\"{date}_{time}_err-pred.geo.json\")\n",
    "\n",
    "# Load Road mask\n",
    "road_ = load_road_mask(MASK_PATH).astype(np.int8)\n",
    "\n",
    "# Interpolate Coords\n",
    "xv, yv = interpolate_coords((1, 115, 512, 448))\n",
    "\n",
    "\n",
    "mask = road_ ==1\n",
    "output = np.concatenate([xv[mask][:,None],\n",
    "                         yv[mask][:,None],\n",
    "                        ], axis=1)\n",
    "output_result = result[:,mask].transpose()\n",
    "\n",
    "# Loop through grids and create polygons\n",
    "polygons = []\n",
    "for rec in output:\n",
    "    x_left = rec[0]\n",
    "    y_up = rec[1]\n",
    "    x_right = x_left + 0.001\n",
    "    y_bot = y_up - 0.001\n",
    "    attr = rec[2:]\n",
    "    polygon = Polygon([(x_left,y_up), (x_right, y_up), (x_right,y_bot),(x_left, y_bot)])\n",
    "    polygons.append(polygon)\n",
    "\n",
    "# Create geo dataframe\n",
    "gdf_data = {'geometry':polygons}\n",
    "for i in range(output_result.shape[1]):\n",
    "    gdf_data[i] = output_result[:,i]\n",
    "grid = gpd.GeoDataFrame(gdf_data,crs=\"EPSG:4326\")\n",
    "\n",
    "st = T.time()\n",
    "# Save\n",
    "data = grid.to_json(show_bbox=True, na=\"drop\")\n",
    "with open(output_result_path,\"w\") as f:\n",
    "    f.writelines(data)\n",
    "print(f\"Saving used {T.time()-st} s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "5e235424",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving used 25.5755033493042 s\n"
     ]
    }
   ],
   "source": [
    "# incident in input\n",
    "date = \"2019-08-30\"\n",
    "time = \"75\"\n",
    "result_path = f\"{date}_berlin_9ch{time}-gt-inc.npy\"\n",
    "result = np.load(os.path.join(log_root, f\"{date}_{time}\", result_path))\n",
    "\n",
    "for i in range(12):\n",
    "    result[i,262:277,259:268] = 45\n",
    "#     result[i,263,254] = 45\n",
    "\n",
    "\n",
    "output_result_path =  os.path.join(output_root, f\"{date}_{time}\", f\"{date}_{time}_gt-inc.geo.json\")\n",
    "\n",
    "# Load Road mask\n",
    "road_ = load_road_mask(MASK_PATH).astype(np.int8)\n",
    "\n",
    "# Interpolate Coords\n",
    "xv, yv = interpolate_coords((1, 115, 512, 448))\n",
    "\n",
    "\n",
    "mask = road_ ==1\n",
    "output = np.concatenate([xv[mask][:,None],\n",
    "                         yv[mask][:,None],\n",
    "                        ], axis=1)\n",
    "output_result = result[:,mask].transpose()\n",
    "\n",
    "# Loop through grids and create polygons\n",
    "polygons = []\n",
    "for rec in output:\n",
    "    x_left = rec[0]\n",
    "    y_up = rec[1]\n",
    "    x_right = x_left + 0.001\n",
    "    y_bot = y_up - 0.001\n",
    "    attr = rec[2:]\n",
    "    polygon = Polygon([(x_left,y_up), (x_right, y_up), (x_right,y_bot),(x_left, y_bot)])\n",
    "    polygons.append(polygon)\n",
    "\n",
    "# Create geo dataframe\n",
    "gdf_data = {'geometry':polygons}\n",
    "for i in range(output_result.shape[1]):\n",
    "    gdf_data[i] = output_result[:,i]\n",
    "grid = gpd.GeoDataFrame(gdf_data,crs=\"EPSG:4326\")\n",
    "\n",
    "st = T.time()\n",
    "# Save\n",
    "data = grid.to_json(show_bbox=True, na=\"drop\")\n",
    "with open(output_result_path,\"w\") as f:\n",
    "    f.writelines(data)\n",
    "print(f\"Saving used {T.time()-st} s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "670398f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([45., 45., 45., 45., 45., 45., 45., 45., 45., 45., 45., 45.],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result[:,262,259]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eace3801",
   "metadata": {},
   "source": [
    "## Save as json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "68843755",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(86, 512, 448)\n",
      "zoom to [294:378, 84:252]\n",
      "(86, 84, 168)\n",
      "(84, 168)\n"
     ]
    }
   ],
   "source": [
    "log_root = r\"C:\\Users\\jingyli\\OwnDrive\\IPA\\attribution_Result\\unet\\attribution_pickle\\resUnet\"\n",
    "MASK_PATH = r\"C:\\Users\\jingyli\\OwnDrive\\IPA\\python-eda-code\\utils\\Berlin.mask\"\n",
    "output_root = r\"C:\\Users\\jingyli\\OwnDrive\\IPA\\web_json\"\n",
    "window_size = 21\n",
    "\n",
    "\n",
    "time = 84\n",
    "date = \"2019-09-19\"\n",
    "\n",
    "\n",
    "window = [16, 8]  # [y,x]\n",
    "file_path = f\"{date}_berlin_9ch{time}-saliency-target-channel0-W{window[0]}-{window[1]}.npy\"\n",
    "file1_path = f\"{date}_berlin_9ch{time}-saliency-target-channel1-W{window[0]}-{window[1]}.npy\"\n",
    "# OUTPUT PATH\n",
    "output_path = os.path.join(output_root, f\"{date}_{time}\")\n",
    "if not os.path.exists(output_path):\n",
    "    os.mkdir(output_path)\n",
    "output_filename = os.path.join(output_path, f\"{date}_{time}_W{window[0]}-{window[1]}.json\")\n",
    "\n",
    "\n",
    "# Attribution to volume / speed\n",
    "attr = np.load(os.path.join(log_root, f\"{date}_{time}\", file_path))\n",
    "attr1 = np.load(os.path.join(log_root, f\"{date}_{time}\", file1_path))\n",
    "\n",
    "# Aggregate attr\n",
    "attr_all = spatial_attribution_agg_flat(attr)\n",
    "attr_all1 = spatial_attribution_agg_flat(attr1)\n",
    "\n",
    "## Add up volume/speed\n",
    "attr_all = np.concatenate([attr_all,attr_all1])\n",
    "print(attr_all.shape)\n",
    "\n",
    "# Zoom to a small area\n",
    "zoom_window_num = 2\n",
    "(start_x, end_x, start_y, end_y) = zoom_to_range(window, window_size, zoom_window_num, attr_all.shape)\n",
    "print(f\"zoom to [{start_x}:{end_x}, {start_y}:{end_y}]\")\n",
    "attr_zoom = attr_all[:, \n",
    "                     start_x : end_x,\n",
    "                     start_y : end_y]\n",
    "print(attr_zoom.shape)\n",
    "\n",
    "# Load Road mask\n",
    "road_ = load_road_mask(MASK_PATH)\n",
    "\n",
    "road_zoom = road_[start_x : end_x,\n",
    "                     start_y : end_y]\n",
    "road_zoom = road_zoom.astype(np.int8)\n",
    "print(road_zoom.shape)\n",
    "\n",
    "# Interpolate Coords\n",
    "xv, yv = interpolate_coords((1, 115, 512, 448))\n",
    "\n",
    "# Apply road mask; Save road attribution to json\n",
    "xv_zoom = xv[start_x : end_x,\n",
    "                     start_y : end_y]\n",
    "yv_zoom = yv[start_x : end_x,\n",
    "                     start_y : end_y]\n",
    "mask = road_zoom ==1\n",
    "output = np.concatenate([xv_zoom[mask][:,None],\n",
    "                         yv_zoom[mask][:,None],\n",
    "                         attr_zoom[:,mask].transpose()\n",
    "                        ], axis=1)\n",
    "output_json = json.dumps(output.tolist())\n",
    "\n",
    "with open(output_filename,\"w\") as f:\n",
    "    f.writelines(output_json)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c176fa05",
   "metadata": {},
   "source": [
    "v_pred,\n",
    "s_pred,\n",
    "v_gt_epoch-v_pred,\n",
    "s_gt_epoch-s_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3e2a11c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prediction and error\n",
    "result_path = f\"{date}_berlin_9ch{time}-err-pred.npy\"\n",
    "result = np.load(os.path.join(log_root, f\"{date}_{time}\", result_path))\n",
    "output_result_path =  os.path.join(log_root, f\"{date}_{time}\", f\"{date}_{time}_pred-err.json\")\n",
    "\n",
    "# Load Road mask\n",
    "road_ = load_road_mask(MASK_PATH).astype(np.int8)\n",
    "\n",
    "# Interpolate Coords\n",
    "xv, yv = interpolate_coords((1, 115, 512, 448))\n",
    "\n",
    "\n",
    "mask = road_ ==1\n",
    "output = np.concatenate([xv[mask][:,None],\n",
    "                         yv[mask][:,None],\n",
    "                         result[:,mask].transpose()\n",
    "                        ], axis=1)\n",
    "\n",
    "output_json = json.dumps(output.tolist())\n",
    "\n",
    "with open(output_result_path,\"w\") as f:\n",
    "    f.writelines(output_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "192809df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# incident in input\n",
    "result_path = f\"{date}_berlin_9ch{time}-gt-inc.npy\"\n",
    "result = np.load(os.path.join(log_root, f\"{date}_{time}\", result_path))\n",
    "output_result_path =  os.path.join(log_root, f\"{date}_{time}\", f\"{date}_{time}_gt-inc.json\")\n",
    "\n",
    "# Load Road mask\n",
    "road_ = load_road_mask(MASK_PATH).astype(np.int8)\n",
    "\n",
    "# Interpolate Coords\n",
    "xv, yv = interpolate_coords((1, 115, 512, 448))\n",
    "\n",
    "\n",
    "mask = road_ ==1\n",
    "output = np.concatenate([xv[mask][:,None],\n",
    "                         yv[mask][:,None],\n",
    "                         result[:,mask].transpose()\n",
    "                        ], axis=1)\n",
    "\n",
    "output_json = json.dumps(output.tolist())\n",
    "\n",
    "with open(output_result_path,\"w\") as f:\n",
    "    f.writelines(output_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cb4054a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.5882353 , 0.5882353 , 0.5882353 , ..., 0.19607843, 1.        ,\n",
       "       0.39215687], dtype=float32)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result[result!=0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2372fe63",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:Geoprocessing]",
   "language": "python",
   "name": "conda-env-Geoprocessing-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
